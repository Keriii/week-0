{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for topic  modeling\n",
    "- data loading\n",
    "- data preprocesing\n",
    "    - Tokenization\n",
    "    - All stopwords are removed.\n",
    "- Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets define the path for week 8 and week 9\n",
    "week_8_path = \"C:/Users/ok/Desktop/Week_0/week-0/data/channels/weeks/all-week8\"\n",
    "week_9_path = \"C:/Users/ok/Desktop/Week_0/week-0/data/channels/weeks/all-week9\"\n",
    "\n",
    "#now lets extract the data from the all-week8 and all-week9 folders\n",
    "week_8 = utils.slack_parser(week_8_path)\n",
    "week_9 = utils.slack_parser(week_9_path)\n",
    "# loading the data\n",
    "data = pd.concat([week_8, week_9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = data['msg_content'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [good, morn, blush, welcom, week, wish, produc...\n",
       "1                                           [good, morn]\n",
       "2                                           [good, morn]\n",
       "3                                           [good, morn]\n",
       "4                                           [good, morn]\n",
       "                             ...                        \n",
       "105                                                   []\n",
       "106                             [thank, python, version]\n",
       "107    [https, realpython, linear, program, python, h...\n",
       "108    [https, stackoverflow, question, distanc, geog...\n",
       "109    [ujgp, nlsu, present, olutosin, kill, super, i...\n",
       "Name: msg_content, Length: 189, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_2'] = processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_type</th>\n",
       "      <th>msg_content</th>\n",
       "      <th>sender_name</th>\n",
       "      <th>msg_sent_time</th>\n",
       "      <th>msg_dist_type</th>\n",
       "      <th>time_thread_start</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>reply_users_count</th>\n",
       "      <th>reply_users</th>\n",
       "      <th>tm_thread_end</th>\n",
       "      <th>channel</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleaned_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message</td>\n",
       "      <td>Good morning everyone :blush: welcome to week ...</td>\n",
       "      <td>Garrett Bell</td>\n",
       "      <td>1665385707.569729</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week8</td>\n",
       "      <td>Good morning everyone blush welcome to week 8 ...</td>\n",
       "      <td>[good, morn, blush, welcom, week, wish, produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message</td>\n",
       "      <td>Good morning!</td>\n",
       "      <td>Carlos Gross</td>\n",
       "      <td>1665385734.616309</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week8</td>\n",
       "      <td>Good morning</td>\n",
       "      <td>[good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>message</td>\n",
       "      <td>Good Morning.</td>\n",
       "      <td>Samuel King</td>\n",
       "      <td>1665385760.620169</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week8</td>\n",
       "      <td>Good Morning</td>\n",
       "      <td>[good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>message</td>\n",
       "      <td>Good Morning!!!</td>\n",
       "      <td>Daniel Brown</td>\n",
       "      <td>1665387214.795849</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week8</td>\n",
       "      <td>Good Morning</td>\n",
       "      <td>[good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>message</td>\n",
       "      <td>Good Morning.</td>\n",
       "      <td>Willie Yang</td>\n",
       "      <td>1665388127.826899</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week8</td>\n",
       "      <td>Good Morning</td>\n",
       "      <td>[good, morn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>message</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Judith Bolton</td>\n",
       "      <td>1666519434.902969</td>\n",
       "      <td>text</td>\n",
       "      <td>1666453751.515939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week9</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>message</td>\n",
       "      <td>Thank you, my python version was 3.10</td>\n",
       "      <td>Travis Butler</td>\n",
       "      <td>1666521758.493219</td>\n",
       "      <td>text</td>\n",
       "      <td>1666453751.515939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week9</td>\n",
       "      <td>Thank you my python version was 310</td>\n",
       "      <td>[thank, python, version]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>message</td>\n",
       "      <td>&lt;https://realpython.com/linear-programming-pyt...</td>\n",
       "      <td>Kelly Soto</td>\n",
       "      <td>1666544242.275809</td>\n",
       "      <td>link</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week9</td>\n",
       "      <td>httpsrealpythoncomlinearprogrammingpythonHands...</td>\n",
       "      <td>[https, realpython, linear, program, python, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>message</td>\n",
       "      <td>&lt;https://stackoverflow.com/questions/70941094/...</td>\n",
       "      <td>Joshua Rhodes</td>\n",
       "      <td>1666552319.011779</td>\n",
       "      <td>link</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week9</td>\n",
       "      <td>httpsstackoverflowcomquestions70941094howtoget...</td>\n",
       "      <td>[https, stackoverflow, question, distanc, geog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>message</td>\n",
       "      <td>&lt;@U03UJGP0C68&gt; &lt;@U03UUR571A5&gt; &lt;@U03V785NLSU&gt; p...</td>\n",
       "      <td>Katherine Foster</td>\n",
       "      <td>1666962823.974699</td>\n",
       "      <td>user</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>all-week9</td>\n",
       "      <td>U03UJGP0C68 U03UUR571A5 U03V785NLSU presented ...</td>\n",
       "      <td>[ujgp, nlsu, present, olutosin, kill, super, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    msg_type                                        msg_content  \\\n",
       "0    message  Good morning everyone :blush: welcome to week ...   \n",
       "1    message                                      Good morning!   \n",
       "2    message                                      Good Morning.   \n",
       "3    message                                    Good Morning!!!   \n",
       "4    message                                      Good Morning.   \n",
       "..       ...                                                ...   \n",
       "105  message                                                3.8   \n",
       "106  message              Thank you, my python version was 3.10   \n",
       "107  message  <https://realpython.com/linear-programming-pyt...   \n",
       "108  message  <https://stackoverflow.com/questions/70941094/...   \n",
       "109  message  <@U03UJGP0C68> <@U03UUR571A5> <@U03V785NLSU> p...   \n",
       "\n",
       "          sender_name      msg_sent_time msg_dist_type  time_thread_start  \\\n",
       "0        Garrett Bell  1665385707.569729          text                  0   \n",
       "1        Carlos Gross  1665385734.616309          text                  0   \n",
       "2         Samuel King  1665385760.620169          text                  0   \n",
       "3        Daniel Brown  1665387214.795849          text                  0   \n",
       "4         Willie Yang  1665388127.826899          text                  0   \n",
       "..                ...                ...           ...                ...   \n",
       "105     Judith Bolton  1666519434.902969          text  1666453751.515939   \n",
       "106     Travis Butler  1666521758.493219          text  1666453751.515939   \n",
       "107        Kelly Soto  1666544242.275809          link                  0   \n",
       "108     Joshua Rhodes  1666552319.011779          link                  0   \n",
       "109  Katherine Foster  1666962823.974699          user                  0   \n",
       "\n",
       "     reply_count  reply_users_count reply_users tm_thread_end    channel  \\\n",
       "0              0                  0           0             0  all-week8   \n",
       "1              0                  0           0             0  all-week8   \n",
       "2              0                  0           0             0  all-week8   \n",
       "3              0                  0           0             0  all-week8   \n",
       "4              0                  0           0             0  all-week8   \n",
       "..           ...                ...         ...           ...        ...   \n",
       "105            0                  0           0             0  all-week9   \n",
       "106            0                  0           0             0  all-week9   \n",
       "107            0                  0           0             0  all-week9   \n",
       "108            0                  0           0             0  all-week9   \n",
       "109            0                  0           0             0  all-week9   \n",
       "\n",
       "                                               cleaned  \\\n",
       "0    Good morning everyone blush welcome to week 8 ...   \n",
       "1                                         Good morning   \n",
       "2                                         Good Morning   \n",
       "3                                         Good Morning   \n",
       "4                                         Good Morning   \n",
       "..                                                 ...   \n",
       "105                                                 38   \n",
       "106                Thank you my python version was 310   \n",
       "107  httpsrealpythoncomlinearprogrammingpythonHands...   \n",
       "108  httpsstackoverflowcomquestions70941094howtoget...   \n",
       "109  U03UJGP0C68 U03UUR571A5 U03V785NLSU presented ...   \n",
       "\n",
       "                                             cleaned_2  \n",
       "0    [good, morn, blush, welcom, week, wish, produc...  \n",
       "1                                         [good, morn]  \n",
       "2                                         [good, morn]  \n",
       "3                                         [good, morn]  \n",
       "4                                         [good, morn]  \n",
       "..                                                 ...  \n",
       "105                                                 []  \n",
       "106                           [thank, python, version]  \n",
       "107  [https, realpython, linear, program, python, h...  \n",
       "108  [https, stackoverflow, question, distanc, geog...  \n",
       "109  [ujgp, nlsu, present, olutosin, kill, super, i...  \n",
       "\n",
       "[189 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessor(data):\n",
    "    \n",
    "    #this function preprocesses the given data's message and create a new column 'cleaned' for topic and sentiment analysis\n",
    "    data['cleaned'] = data['msg_content'].astype(str) #convertt to string\n",
    "    data['cleaned'] = data['msg_content'].apply(lambda x: x.lower()) #convert to lower case\n",
    "    data['cleaned'] = data['msg_content'].apply(lambda x: re.sub(r'http\\S+', '', x)) # to remove links\n",
    "    data['cleaned']= data['msg_content'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation))) #remove punctuations\n",
    "\n",
    "    return data\n",
    "\n",
    "def features(data):\n",
    "\n",
    "    #this function changes sentences into list of words\n",
    "    messages = [mess for mess in data['cleaned']]\n",
    "    words = [mess.split() for mess in messages]\n",
    "\n",
    "    #create dictionalry that containd ID and words \n",
    "    word_to_id = corpora.Dictionary(words) #generate unique tokens\n",
    "    #  we can see the word to unique integer mapping\n",
    "    # print(word_to_id.token2id)\n",
    "    # using bag of words(bow), we create a corpus that contains the word id and its frequency in each document.\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in words]\n",
    "    # TFIDF\n",
    "\n",
    "    return data,words, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2(data):\n",
    "\n",
    "    #this function changes sentences into list of words\n",
    "    messages = [mess for mess in data['cleaned2']]\n",
    "    words = [mess.split() for mess in messages]\n",
    "\n",
    "    #create dictionalry that containd ID and words \n",
    "    word_to_id = corpora.Dictionary(words) #generate unique tokens\n",
    "    #  we can see the word to unique integer mapping\n",
    "    # print(word_to_id.token2id)\n",
    "    # using bag of words(bow), we create a corpus that contains the word id and its frequency in each document.\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in words]\n",
    "    # TFIDF\n",
    "\n",
    "    return data,words, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_preprocessor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = features(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, words, word2id, corous = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = [[(word2id[id], count) for id, count in line] for line in corous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corous,\n",
    "                                           id2word=word2id,\n",
    "                                           num_topics=5,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('the', 0.030163215),\n",
      "   ('Thank', 0.021695467),\n",
      "   ('you', 0.015101622),\n",
      "   ('or', 0.014796451),\n",
      "   ('a', 0.014373034),\n",
      "   ('in', 0.01290342),\n",
      "   ('my', 0.012501105),\n",
      "   ('of', 0.012345727),\n",
      "   ('time', 0.011759203),\n",
      "   ('I', 0.01157879)]),\n",
      " (1,\n",
      "  [('the', 0.0617256),\n",
      "   ('and', 0.022809105),\n",
      "   ('to', 0.017554106),\n",
      "   ('of', 0.016620956),\n",
      "   ('we', 0.016083663),\n",
      "   ('can', 0.015224413),\n",
      "   ('trip', 0.014586046),\n",
      "   ('I', 0.013482074),\n",
      "   ('for', 0.013442016),\n",
      "   ('that', 0.013403673)]),\n",
      " (2,\n",
      "  [('it', 0.04236458),\n",
      "   ('to', 0.019103276),\n",
      "   ('is', 0.016692026),\n",
      "   ('but', 0.012356782),\n",
      "   ('I', 0.009710267),\n",
      "   ('the', 0.009648425),\n",
      "   ('of', 0.009419828),\n",
      "   ('was', 0.0094194235),\n",
      "   ('and', 0.009313228),\n",
      "   ('in', 0.009055061)]),\n",
      " (3,\n",
      "  [('the', 0.05654822),\n",
      "   ('you', 0.026770767),\n",
      "   ('to', 0.025092196),\n",
      "   ('and', 0.0187004),\n",
      "   ('of', 0.018077752),\n",
      "   ('on', 0.015081719),\n",
      "   ('I', 0.014895126),\n",
      "   ('what', 0.011050259),\n",
      "   ('submission', 0.010970979),\n",
      "   ('is', 0.010945489)]),\n",
      " (4,\n",
      "  [('the', 0.030138012),\n",
      "   ('is', 0.020629836),\n",
      "   ('to', 0.0144426245),\n",
      "   ('with', 0.012833963),\n",
      "   ('it', 0.0123114055),\n",
      "   ('an', 0.012088243),\n",
      "   ('in', 0.011910724),\n",
      "   ('What', 0.011502686),\n",
      "   ('you', 0.011395593),\n",
      "   ('this', 0.010859802)])]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
