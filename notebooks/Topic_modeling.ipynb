{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get started by importing the new data\n",
    "data = pd.read_csv('C:/Users/ok/Desktop/Week_0/week-0/data/all_data.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the message got further cleaning\n",
    "data['cleaned'] = data['msg_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessor(data):\n",
    "    \n",
    "    #this function preprocesses the given data's message and create a new column 'cleaned' for topic and sentiment analysis\n",
    "    data['cleaned'] = data['cleaned'].astype(str) #convertt to string\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: x.lower()) #convert to lower case\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'http\\S+', '', x)) # to remove links\n",
    "    data['cleaned']= data['cleaned'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation))) #remove punctuations\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if len(word)>3])) #remove words with length less than 3\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def features(data):\n",
    "\n",
    "    #this function changes sentences into list of words\n",
    "    messages = [mess for mess in data['cleaned']]\n",
    "    words = [mess.split() for mess in messages]\n",
    "\n",
    "    #create dictionalry that containd ID and words \n",
    "    word_to_id = corpora.Dictionary(words) #generate unique tokens\n",
    "    #  we can see the word to unique integer mapping\n",
    "    # print(word_to_id.token2id)\n",
    "    # using bag of words(bow), we create a corpus that contains the word id and its frequency in each document.\n",
    "    corpus_1= [word_to_id.doc2bow(word) for word in words]\n",
    "    # TFIDF\n",
    "\n",
    "    return data,words, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the message content\n",
    "data1 = data_preprocessor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the cleaned data into features\n",
    "data2 = features(data1)\n",
    "data, words, word2id, corous = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = [[(word2id[id], count) for id, count in line] for line in corous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corous,\n",
    "                                           id2word=word2id,\n",
    "                                           num_topics=5,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('task', 0.03700923),\n",
      "   ('data', 0.035471413),\n",
      "   ('using', 0.021934109),\n",
      "   ('great', 0.016249854),\n",
      "   ('done', 0.014091503),\n",
      "   ('know', 0.0134693505),\n",
      "   ('could', 0.012234431),\n",
      "   ('would', 0.012232722),\n",
      "   ('already', 0.010701083),\n",
      "   ('check', 0.009572877)]),\n",
      " (1,\n",
      "  [('time', 0.060443614),\n",
      "   ('working', 0.040980726),\n",
      "   ('please', 0.036029268),\n",
      "   ('right', 0.03128182),\n",
      "   ('google', 0.019797508),\n",
      "   ('rollingonthefloorlaughing', 0.018444775),\n",
      "   ('call', 0.017624373),\n",
      "   ('today', 0.01565977),\n",
      "   ('mean', 0.01544796),\n",
      "   ('meet', 0.015406777)]),\n",
      " (2,\n",
      "  [('work', 0.02618495),\n",
      "   ('link', 0.023628084),\n",
      "   ('thanks', 0.021462074),\n",
      "   ('dont', 0.01943454),\n",
      "   ('like', 0.018558834),\n",
      "   ('repo', 0.017506672),\n",
      "   ('going', 0.014438911),\n",
      "   ('tasks', 0.01326881),\n",
      "   ('need', 0.0127672255),\n",
      "   ('lets', 0.011932013)]),\n",
      " (3,\n",
      "  [('think', 0.041726463),\n",
      "   ('guys', 0.0339931),\n",
      "   ('meeting', 0.026752032),\n",
      "   ('good', 0.019560965),\n",
      "   ('also', 0.017084975),\n",
      "   ('create', 0.01696764),\n",
      "   ('file', 0.015833983),\n",
      "   ('group', 0.014603719),\n",
      "   ('problem', 0.012174424),\n",
      "   ('uujgrne', 0.012029491)]),\n",
      " (4,\n",
      "  [('error', 0.02666567),\n",
      "   ('sure', 0.02623094),\n",
      "   ('okay', 0.021271132),\n",
      "   ('yeah', 0.019553024),\n",
      "   ('thank', 0.01721673),\n",
      "   ('send', 0.016365327),\n",
      "   ('hello', 0.014410391),\n",
      "   ('start', 0.014073467),\n",
      "   ('well', 0.013608503),\n",
      "   ('didnt', 0.013594304)])]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
