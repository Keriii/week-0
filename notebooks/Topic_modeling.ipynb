{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get started by importing the new data\n",
    "data = pd.read_csv('C:/Users/ok/Desktop/Week_0/week-0/data/all_data.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "sentiments = data['msg_content'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "# Add the sentiment scores as a new column in the dataframe\n",
    "data['sentiment_score'] = sentiments\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative cleaning function\n",
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessor(data):\n",
    "    \n",
    "    #this function preprocesses the given data's message and create a new column 'cleaned' for topic and sentiment analysis\n",
    "    data['cleaned'] = data['msg_content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()])) #remove numbers\n",
    "    \n",
    "    data['cleaned'] = data['cleaned'].astype(str) #convertt to string\n",
    "    \n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: x.lower()) #convert to lower case\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: re.sub(r'http\\S+', '', x)) # to remove links\n",
    "    data['cleaned']= data['cleaned'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation))) #remove punctuations\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n",
    "    data['cleaned'] = data['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if len(word)>3])) #remove words with length less than 3\n",
    "\n",
    "    return data\n",
    "\n",
    "def features(data):\n",
    "\n",
    "    #this function changes sentences into list of words\n",
    "    messages = [mess for mess in data['cleaned']]\n",
    "    words = [mess.split() for mess in messages]\n",
    "\n",
    "    #create dictionalry that containd ID and words \n",
    "    word_to_id = corpora.Dictionary(words) #generate unique tokens\n",
    "    #  we can see the word to unique integer mapping\n",
    "    # print(word_to_id.token2id)\n",
    "    # using bag of words(bow), we create a corpus that contains the word id and its frequency in each document.\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in words]\n",
    "    # TFIDF\n",
    "\n",
    "    return data,words, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessor2(data):\n",
    "    if isinstance(data, float):\n",
    "        return ''\n",
    "    \n",
    "    # rest of the code\n",
    "    stop_free = ' '.join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    \n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\Week_0\\week-0\\notebooks\\Topic_modeling.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m processed_docs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mmsg_content\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(clean)\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4539\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\n\u001b[0;32m   4461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4462\u001b[0m     arg: Callable \u001b[39m|\u001b[39m Mapping \u001b[39m|\u001b[39m Series,\n\u001b[0;32m   4463\u001b[0m     na_action: Literal[\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   4464\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[0;32m   4465\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4466\u001b[0m \u001b[39m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4467\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4537\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[0;32m   4538\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4539\u001b[0m     new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_values(arg, na_action\u001b[39m=\u001b[39;49mna_action)\n\u001b[0;32m   4540\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_values, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m.\u001b[39m__finalize__(\n\u001b[0;32m   4541\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4542\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:890\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    889\u001b[0m \u001b[39m# mapper is a function\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m new_values \u001b[39m=\u001b[39m map_f(values, mapper)\n\u001b[0;32m    892\u001b[0m \u001b[39mreturn\u001b[39;00m new_values\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\Week_0\\week-0\\notebooks\\Topic_modeling.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean\u001b[39m(doc):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     stop_free \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39;49mlower()\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     punc_free \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([ch \u001b[39mfor\u001b[39;00m ch \u001b[39min\u001b[39;00m stop_free \u001b[39mif\u001b[39;00m ch \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     normalized \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lemma\u001b[39m.\u001b[39mlemmatize(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m punc_free\u001b[39m.\u001b[39msplit())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "processed_docs = data['msg_content'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [good, morn, blush, welcom, week, wish, produc...\n",
       "1                                           [good, morn]\n",
       "2                                           [good, morn]\n",
       "3                                           [good, morn]\n",
       "4                                           [good, morn]\n",
       "                             ...                        \n",
       "105                                                   []\n",
       "106                             [thank, python, version]\n",
       "107    [https, realpython, linear, program, python, h...\n",
       "108    [https, stackoverflow, question, distanc, geog...\n",
       "109    [ujgp, nlsu, present, olutosin, kill, super, i...\n",
       "Name: msg_content, Length: 189, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_2'] = processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2(data):\n",
    "\n",
    "    #this function changes sentences into list of words\n",
    "    messages = [mess for mess in data['cleaned2']]\n",
    "    words = [mess.split() for mess in messages]\n",
    "\n",
    "    #create dictionalry that containd ID and words \n",
    "    word_to_id = corpora.Dictionary(words) #generate unique tokens\n",
    "    #  we can see the word to unique integer mapping\n",
    "    # print(word_to_id.token2id)\n",
    "    # using bag of words(bow), we create a corpus that contains the word id and its frequency in each document.\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in words]\n",
    "    # TFIDF\n",
    "\n",
    "    return data,words, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\Week_0\\week-0\\notebooks\\Topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data1 \u001b[39m=\u001b[39m data_preprocessor(data)\n",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\Week_0\\week-0\\notebooks\\Topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_preprocessor\u001b[39m(data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#this function preprocesses the given data's message and create a new column 'cleaned' for topic and sentiment analysis\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m#convertt to string\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mmsg_content\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mlower()) \u001b[39m#convert to lower case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, x)) \u001b[39m# to remove links\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mtranslate(\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, string\u001b[39m.\u001b[39mpunctuation))) \u001b[39m#remove punctuations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\Week_0\\week-0\\notebooks\\Topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_preprocessor\u001b[39m(data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#this function preprocesses the given data's message and create a new column 'cleaned' for topic and sentiment analysis\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m#convertt to string\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mlower()) \u001b[39m#convert to lower case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, x)) \u001b[39m# to remove links\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/Week_0/week-0/notebooks/Topic_modeling.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmsg_content\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mtranslate(\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, string\u001b[39m.\u001b[39mpunctuation))) \u001b[39m#remove punctuations\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "data1 = data_preprocessor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = features(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, words, word2id, corous = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = [[(word2id[id], count) for id, count in line] for line in corous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corous,\n",
    "                                           id2word=word2id,\n",
    "                                           num_topics=5,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('the', 0.030163215),\n",
      "   ('Thank', 0.021695467),\n",
      "   ('you', 0.015101622),\n",
      "   ('or', 0.014796451),\n",
      "   ('a', 0.014373034),\n",
      "   ('in', 0.01290342),\n",
      "   ('my', 0.012501105),\n",
      "   ('of', 0.012345727),\n",
      "   ('time', 0.011759203),\n",
      "   ('I', 0.01157879)]),\n",
      " (1,\n",
      "  [('the', 0.0617256),\n",
      "   ('and', 0.022809105),\n",
      "   ('to', 0.017554106),\n",
      "   ('of', 0.016620956),\n",
      "   ('we', 0.016083663),\n",
      "   ('can', 0.015224413),\n",
      "   ('trip', 0.014586046),\n",
      "   ('I', 0.013482074),\n",
      "   ('for', 0.013442016),\n",
      "   ('that', 0.013403673)]),\n",
      " (2,\n",
      "  [('it', 0.04236458),\n",
      "   ('to', 0.019103276),\n",
      "   ('is', 0.016692026),\n",
      "   ('but', 0.012356782),\n",
      "   ('I', 0.009710267),\n",
      "   ('the', 0.009648425),\n",
      "   ('of', 0.009419828),\n",
      "   ('was', 0.0094194235),\n",
      "   ('and', 0.009313228),\n",
      "   ('in', 0.009055061)]),\n",
      " (3,\n",
      "  [('the', 0.05654822),\n",
      "   ('you', 0.026770767),\n",
      "   ('to', 0.025092196),\n",
      "   ('and', 0.0187004),\n",
      "   ('of', 0.018077752),\n",
      "   ('on', 0.015081719),\n",
      "   ('I', 0.014895126),\n",
      "   ('what', 0.011050259),\n",
      "   ('submission', 0.010970979),\n",
      "   ('is', 0.010945489)]),\n",
      " (4,\n",
      "  [('the', 0.030138012),\n",
      "   ('is', 0.020629836),\n",
      "   ('to', 0.0144426245),\n",
      "   ('with', 0.012833963),\n",
      "   ('it', 0.0123114055),\n",
      "   ('an', 0.012088243),\n",
      "   ('in', 0.011910724),\n",
      "   ('What', 0.011502686),\n",
      "   ('you', 0.011395593),\n",
      "   ('this', 0.010859802)])]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
